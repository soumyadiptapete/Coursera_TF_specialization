{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Week4-Exercise-Shakespeare-Question.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadiptapete/Coursera_TF_specialization/blob/master/NLP_Week4_Exercise_Shakespeare_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dde9b6e-bdac-46ca-d66f-05c38fa46d16"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1ee580bb-2690-4b63-8375-432e701ebac8"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-10 14:47:23--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-09-10 14:47:28 (118 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vH8Y59ajYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d596a237-e7f7-437a-ebb4-792f172db279"
      },
      "source": [
        "embedding_dim=64\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words,embedding_dim,input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(50,return_sequences=True)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(200,activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words,activation='softmax'))\n",
        "# Pick an optimizer\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 10, 64)            205504    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 10, 100)           46000     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 200)               160800    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3211)              645411    \n",
            "=================================================================\n",
            "Total params: 1,097,915\n",
            "Trainable params: 1,097,915\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIg2f1HBxqof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86cacd06-ac0e-461f-f651-ad9e46db07cf"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0910 14:48:23.322309 139820555270016 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "15462/15462 [==============================] - 52s 3ms/sample - loss: 6.9381 - acc: 0.0208\n",
            "Epoch 2/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 6.5264 - acc: 0.0250\n",
            "Epoch 3/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 6.3902 - acc: 0.0344\n",
            "Epoch 4/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 6.2238 - acc: 0.0391\n",
            "Epoch 5/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 6.0272 - acc: 0.0504\n",
            "Epoch 6/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 5.8239 - acc: 0.0623\n",
            "Epoch 7/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 5.6270 - acc: 0.0744\n",
            "Epoch 8/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 5.4309 - acc: 0.0874\n",
            "Epoch 9/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 5.2156 - acc: 0.1016\n",
            "Epoch 10/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 4.9917 - acc: 0.1226\n",
            "Epoch 11/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 4.7695 - acc: 0.1524\n",
            "Epoch 12/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 4.5472 - acc: 0.1845\n",
            "Epoch 13/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 4.3212 - acc: 0.2224\n",
            "Epoch 14/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 4.1049 - acc: 0.2690\n",
            "Epoch 15/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 3.9014 - acc: 0.3053\n",
            "Epoch 16/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 3.7015 - acc: 0.3480\n",
            "Epoch 17/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 3.5145 - acc: 0.3843\n",
            "Epoch 18/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 3.3358 - acc: 0.4191\n",
            "Epoch 19/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 3.1693 - acc: 0.4541\n",
            "Epoch 20/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 3.0069 - acc: 0.4840\n",
            "Epoch 21/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 2.8469 - acc: 0.5137\n",
            "Epoch 22/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 2.7021 - acc: 0.5448\n",
            "Epoch 23/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 2.5543 - acc: 0.5739\n",
            "Epoch 24/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 2.4228 - acc: 0.6017\n",
            "Epoch 25/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 2.2906 - acc: 0.6305\n",
            "Epoch 26/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 2.1686 - acc: 0.6548\n",
            "Epoch 27/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 2.0611 - acc: 0.6726\n",
            "Epoch 28/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.9582 - acc: 0.6953\n",
            "Epoch 29/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.8531 - acc: 0.7164\n",
            "Epoch 30/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.7687 - acc: 0.7270\n",
            "Epoch 31/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.6865 - acc: 0.7436\n",
            "Epoch 32/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.5952 - acc: 0.7564\n",
            "Epoch 33/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 1.5305 - acc: 0.7687\n",
            "Epoch 34/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 1.4716 - acc: 0.7815\n",
            "Epoch 35/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.4115 - acc: 0.7905\n",
            "Epoch 36/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.3530 - acc: 0.7966\n",
            "Epoch 37/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 1.3110 - acc: 0.8036\n",
            "Epoch 38/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.2714 - acc: 0.8071\n",
            "Epoch 39/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.2250 - acc: 0.8154\n",
            "Epoch 40/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.1831 - acc: 0.8219\n",
            "Epoch 41/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.1545 - acc: 0.8235\n",
            "Epoch 42/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.1293 - acc: 0.8253\n",
            "Epoch 43/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.0944 - acc: 0.8289\n",
            "Epoch 44/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.0696 - acc: 0.8335\n",
            "Epoch 45/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.0499 - acc: 0.8310\n",
            "Epoch 46/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.0289 - acc: 0.8339\n",
            "Epoch 47/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.0001 - acc: 0.8364\n",
            "Epoch 48/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.9885 - acc: 0.8373\n",
            "Epoch 49/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.9711 - acc: 0.8407\n",
            "Epoch 50/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.9562 - acc: 0.8392\n",
            "Epoch 51/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.9390 - acc: 0.8436\n",
            "Epoch 52/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.9180 - acc: 0.8434\n",
            "Epoch 53/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.9034 - acc: 0.8449\n",
            "Epoch 54/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8948 - acc: 0.8441\n",
            "Epoch 55/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8857 - acc: 0.8427\n",
            "Epoch 56/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 0.8796 - acc: 0.8425\n",
            "Epoch 57/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.8597 - acc: 0.8460\n",
            "Epoch 58/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8480 - acc: 0.8471\n",
            "Epoch 59/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8353 - acc: 0.8476\n",
            "Epoch 60/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8339 - acc: 0.8480\n",
            "Epoch 61/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8230 - acc: 0.8483\n",
            "Epoch 62/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.8170 - acc: 0.8474\n",
            "Epoch 63/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.8099 - acc: 0.8498\n",
            "Epoch 64/100\n",
            "15462/15462 [==============================] - 50s 3ms/sample - loss: 0.8020 - acc: 0.8456\n",
            "Epoch 65/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7937 - acc: 0.8499\n",
            "Epoch 66/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7886 - acc: 0.8486\n",
            "Epoch 67/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.7818 - acc: 0.8484\n",
            "Epoch 68/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7829 - acc: 0.8487\n",
            "Epoch 69/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7895 - acc: 0.8460\n",
            "Epoch 70/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7607 - acc: 0.8499\n",
            "Epoch 71/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7583 - acc: 0.8481\n",
            "Epoch 72/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.7508 - acc: 0.8497\n",
            "Epoch 73/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.7523 - acc: 0.8487\n",
            "Epoch 74/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7474 - acc: 0.8480\n",
            "Epoch 75/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7419 - acc: 0.8487\n",
            "Epoch 76/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7369 - acc: 0.8494\n",
            "Epoch 77/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7343 - acc: 0.8490\n",
            "Epoch 78/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7373 - acc: 0.8469\n",
            "Epoch 79/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7354 - acc: 0.8482\n",
            "Epoch 80/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7234 - acc: 0.8494\n",
            "Epoch 81/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7158 - acc: 0.8513\n",
            "Epoch 82/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7056 - acc: 0.8504\n",
            "Epoch 83/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.7066 - acc: 0.8500\n",
            "Epoch 84/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.7155 - acc: 0.8490\n",
            "Epoch 85/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.7040 - acc: 0.8500\n",
            "Epoch 86/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.7037 - acc: 0.8493\n",
            "Epoch 87/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6966 - acc: 0.8485\n",
            "Epoch 88/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6978 - acc: 0.8493\n",
            "Epoch 89/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6926 - acc: 0.8507\n",
            "Epoch 90/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.6884 - acc: 0.8494\n",
            "Epoch 91/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.6865 - acc: 0.8501\n",
            "Epoch 92/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6813 - acc: 0.8497\n",
            "Epoch 93/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6825 - acc: 0.8497\n",
            "Epoch 94/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6880 - acc: 0.8494\n",
            "Epoch 95/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6718 - acc: 0.8508\n",
            "Epoch 96/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.6726 - acc: 0.8509\n",
            "Epoch 97/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6716 - acc: 0.8508\n",
            "Epoch 98/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6708 - acc: 0.8501\n",
            "Epoch 99/100\n",
            "15462/15462 [==============================] - 49s 3ms/sample - loss: 0.6636 - acc: 0.8519\n",
            "Epoch 100/100\n",
            "15462/15462 [==============================] - 48s 3ms/sample - loss: 0.6681 - acc: 0.8479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fXTEO3GJ282",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "46f9316e-362e-413b-ea27-d58fdd862a02"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d24ff6072e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}